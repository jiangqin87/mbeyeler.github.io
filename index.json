[{"authors":["**M Beyeler**","EL Rounds","KD Carlson","N Dutt","JL Krichmar"],"categories":null,"content":"","date":1561618800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561921775,"objectID":"5512878ad8b0409cabbe2619484addd5","permalink":"https://mbeyeler.github.io/publication/neural-correlates-sparse-coding-dimensionality-reduction/","publishdate":"2019-06-27T00:00:00-07:00","relpermalink":"/publication/neural-correlates-sparse-coding-dimensionality-reduction/","section":"publication","summary":"Supported by recent computational studies, there is increasing evidence that a wide range of neuronal responses can be understood as an emergent property of nonnegative sparse coding (NSC), an efficient population coding scheme based on dimensionality reduction and sparsity constraints. We review evidence that NSC might be employed by sensory areas to efficiently encode external stimulus spaces, by some associative areas to conjunctively represent multiple behaviorally relevant variables, and possibly by the basal ganglia to coordinate movement. In addition, NSC might provide a useful theoretical framework under which to understand the often complex and nonintuitive response properties of neurons in other brain areas. Although NSC might not apply to all brain areas (for example, motor or executive function areas) the success of NSC-based models, especially in sensory areas, warrants further investigation for neural correlates in other regions.","tags":[],"title":"Neural correlates of sparse coding and dimensionality reduction","type":"publication"},{"authors":["**M Beyeler**","D Nanduri","JD Weiland","A Rokem","GM Boynton","I Fine"],"categories":null,"content":"","date":1561359600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561921775,"objectID":"0bc870e4b7f63aec8b81a89b182bfb9b","permalink":"https://mbeyeler.github.io/publication/axon-map-model/","publishdate":"2019-06-24T00:00:00-07:00","relpermalink":"/publication/axon-map-model/","section":"publication","summary":"Degenerative retinal diseases such as retinitis pigmentosa and macular degeneration cause irreversible vision loss in more than 10 million people worldwide. Retinal prostheses, now implanted in more than 250 patients worldwide, electrically stimulate surviving cells in order to evoke neuronal responses that are interpreted by the brain as visual percepts ('phosphenes'). However, instead of seeing focal spots of light, users of current epiretinal devices perceive highly distorted phosphenes, which vary in shape not just across subjects but also across electrodes, resulting in distorted percepts. We characterized these distortions by asking users of the Argus retinal prosthesis system (Second Sight Medical Products, Inc.) to draw percepts elicited by single-electrode stimulation on a touchscreen. Based on ophthalmic fundus photographs, we then developed a computational model of the topographic organization of optic nerve fiber bundles in each subject's retina, and used this model to successfully simulate predicted patient percepts. Our model shows that activation of passing axon fibers contributes to the rich repertoire of phosphene shapes reported by patients in our psychophysical measurements, successfully replicating visual percepts ranging from 'blobs' to oriented 'streaks' and 'wedges' depending on the retinal location of the stimulating electrode. This model provides a first step towards future devices that incorporate stimulation strategies tailored to each individual patient's retinal neurophysiology.","tags":[],"title":"A model of ganglion axon pathways accounts for percepts elicited by retinal implants","type":"publication"},{"authors":["**M Beyeler**"],"categories":null,"content":"","date":1557471600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557681208,"objectID":"e8769e8158f889239425312586c3ad67","permalink":"https://mbeyeler.github.io/publication/retinal-sheet-transplants/","publishdate":"2019-05-10T00:00:00-07:00","relpermalink":"/publication/retinal-sheet-transplants/","section":"publication","summary":"Retinal degenerative diseases such as retinitis pigmentosa (RP) and age-related macular degeneration (ARMD) are among the leading causes of blindness in the world. These diseases are characterized by a progressive loss of photoreceptors and/or retinal pigment epithelium (RPE), leading to severe remodeling of the retinal circuitry (Marc et al., 2003) and a gradual loss of vision. However, cells in the inner retina that connect to the brain may remain functional throughout the disease (Santos et al., 1997). Therefore, if the diseased cells could be bypassed or replaced with new cells that connect to the functional part of the retina, it might be possible to restore vision in affected individuals. One approach is to replace diseased cells with healthy cells through transplantation ...","tags":[],"title":"Commentary: Detailed visual cortical responses generated by retinal sheet transplants in rats with severe retinal degeneration","type":"publication"},{"authors":["**M Beyeler**"],"categories":null,"content":"","date":1535785200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541791001,"objectID":"a878009f99bdd2530cda5279179e0371","permalink":"https://mbeyeler.github.io/publication/biophysical-model-axonal-stimulation/","publishdate":"2018-09-01T00:00:00-07:00","relpermalink":"/publication/biophysical-model-axonal-stimulation/","section":"publication","summary":"Visual prostheses aim to restore vision to people blinded from degenerative photoreceptor diseases by electrically stimulating surviving neurons in the retina. However, a major challenge with epiretinal prostheses is that they may accidentally activate passing axon fibers, causing severe perceptual distortions. To investigate the effect of axonal stimulation on the retinal response, we developed a computational model of a small population of morphologically and biophysically detailed retinal ganglion cells, and simulated their response to epiretinal electrical stimulation. We found that activation thresholds of ganglion cell somas and axons varied systematically with both stimulus pulse duration and electrode-retina distance. These findings have important implications for the improvement of stimulus encoding methods for epiretinal prostheses.","tags":[],"title":"Biophysical model of axonal stimulation in epiretinal visual prostheses","type":"publication"},{"authors":["T-S Chou","HJ Kashyap","J Xing","S Listopad","EL Rounds","**M Beyeler**","N Dutt","JL Krichmar"],"categories":null,"content":"","date":1531465200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541791001,"objectID":"233745e1a7f4ab8e3d3d2950ce1d2eaa","permalink":"https://mbeyeler.github.io/publication/carlsim4/","publishdate":"2018-07-13T00:00:00-07:00","relpermalink":"/publication/carlsim4/","section":"publication","summary":"Large-scale spiking neural network (SNN) simulations are challenging to implement, due to the memory and computation required to iteratively process the large set of neural state dynamics and updates. To meet these challenges, we have developed CARLsim 4, a user-friendly SNN library written in C++ that can simulate large biologically detailed neural networks. Improving on the efficiency and scalability of earlier releases, the present release allows for the simulation using multiple GPUs and multiple CPU cores concurrently in a heterogeneous computing cluster. Benchmarking results demonstrate simulation of 8.6 million neurons and 0.48 billion synapses using 4 GPUs and up to 60x speedup for multi-GPU implementations over a single-threaded CPU implementation, making CARLsim 4 well-suited for large-scale SNN models in the presence of real-time constraints. Additionally, the present release adds new features, such as leaky-integrate-and-fire (LIF), 9-parameter Izhikevich, multi-compartment neuron models, and fourth order Runge Kutta integration.","tags":[],"title":"CARLsim 4: An open source library for large scale, biologically detailed spiking neural network simulation using heterogeneous clusters","type":"publication"},{"authors":["**M Beyeler**","A Rokem","GM Boynton","I Fine"],"categories":null,"content":"","date":1503385200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541791001,"objectID":"0e8e14628dc7d72f65b1a80243da7ed9","permalink":"https://mbeyeler.github.io/publication/learning-to-see-again/","publishdate":"2017-08-22T00:00:00-07:00","relpermalink":"/publication/learning-to-see-again/","section":"publication","summary":"The 'bionic eye'—so long a dream of the future—is finally becoming a reality with retinal prostheses available to patients in both the US and Europe. However, clinical experience with these implants has made it apparent that the visual information provided by these devices differs substantially from normal sight. Consequently, the ability of patients to learn to make use of this abnormal retinal input plays a critical role in whether or not some functional vision is successfully regained. The goal of the present review is to summarize the vast basic science literature on developmental and adult cortical plasticity with an emphasis on how this literature might relate to the field of prosthetic vision. We begin with describing the distortion and information loss likely to be experienced by visual prosthesis users. We then define cortical plasticity and perceptual learning, and describe what is known, and what is unknown, about visual plasticity across the hierarchy of brain regions involved in visual processing, and across different stages of life. We close by discussing what is known about brain plasticity in sight restoration patients and discuss biological mechanisms that might eventually be harnessed to improve visual learning in these patients.","tags":[],"title":"Learning to see again: Biological constraints on cortical plasticity and the implications for sight restoration technologies","type":"publication"},{"authors":["**M Beyeler**","GM Boynton","I Fine","A Rokem"],"categories":null,"content":"","date":1498892400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541791001,"objectID":"b5611c38db37a9e44e577e58b799d70e","permalink":"https://mbeyeler.github.io/publication/pulse2percept/","publishdate":"2017-07-01T00:00:00-07:00","relpermalink":"/publication/pulse2percept/","section":"publication","summary":"By 2020 roughly 20 million people worldwide will suffer from photoreceptor diseases such as retinitis pigmentosa and age-related macular degeneration, and a variety of retinal sight restoration technologies are being developed to target these diseases. One technology, analogous to cochlear implants, uses a grid of electrodes to stimulate remaining retinal cells. Two brands of retinal prostheses are currently approved for implantation in patients with late stage photoreceptor disease. Clinical experience with these implants has made it apparent that the vision restored by these devices differs substantially from normal sight. To better understand the outcomes of this technology, we developed pulse2percept, an open-source Python implementation of a computational model that predicts the perceptual experience of retinal prosthesis patients across a wide range of implant configurations. A modular and extensible user interface exposes the different building blocks of the software, making it easy for users to simulate novel implants, stimuli, and retinal models. We hope that this library will contribute substantially to the field of medicine by providing a tool to accelerate the development of visual prostheses.","tags":[],"title":"pulse2percept: A Python-based simulation framework for bionic vision","type":"publication"},{"authors":["**M Beyeler**","N Dutt","JL Krichmar"],"categories":null,"content":"","date":1470034800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541791001,"objectID":"867784d9c1445109d2cde529ea762a25","permalink":"https://mbeyeler.github.io/publication/sparse-decomposition-model/","publishdate":"2016-08-01T00:00:00-07:00","relpermalink":"/publication/sparse-decomposition-model/","section":"publication","summary":"Neurons in the dorsal subregion of the medial superior temporal (MSTd) area of the macaque respond to large, complex patterns of retinal flow, implying a role in the analysis of self-motion. Some neurons are selective for the expanding radial motion that occurs as an observer moves through the environment (“heading”), and computational models can account for this finding. However, ample evidence suggests that MSTd neurons exhibit a continuum of visual response selectivity to large-field motion stimuli. Furthermore, the underlying computational principles by which these response properties are derived remain poorly understood. Here we describe a computational model of macaque MSTd based on the hypothesis that neurons in MSTd efficiently encode the continuum of large-field retinal flow patterns on the basis of inputs received from neurons in MT with receptive fields that resemble basis vectors recovered with non-negative matrix factorization. These assumptions are sufficient to quantitatively simulate neurophysiological response properties of MSTd cells, such as 3D translation and rotation selectivity, suggesting that these properties might simply be a byproduct of MSTd neurons performing dimensionality reduction on their inputs. At the population level, model MSTd accurately predicts eye velocity and heading using a sparse distributed code, consistent with the idea that biological MSTd might be well equipped to efficiently encode various self-motion variables. The present work aims to add some structure to the often contradictory findings about macaque MSTd, and offers a biologically plausible account of a wide range of visual response properties ranging from single-unit selectivity to population statistics.","tags":[],"title":"3D visual response properties of MSTd emerge from an efficient, sparse population code","type":"publication"},{"authors":["**M Beyeler**","N Oros","N Dutt","JL Krichmar"],"categories":null,"content":"","date":1448956800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541791001,"objectID":"f2d45155dbf8bfaa49ab86832e7c67af","permalink":"https://mbeyeler.github.io/publication/gpu-visually-guided-robot-navigation/","publishdate":"2015-12-01T00:00:00-08:00","relpermalink":"/publication/gpu-visually-guided-robot-navigation/","section":"publication","summary":"Humans and other terrestrial animals use vision to traverse novel cluttered environments with apparent ease. On one hand, although much is known about the behavioral dynamics of steering in humans, it remains unclear how relevant perceptual variables might be represented in the brain. On the other hand, although a wealth of data exists about the neural circuitry that is concerned with the perception of self-motion variables such as the current direction of travel, little research has been devoted to investigating how this neural circuitry may relate to active steering control. Here we present a cortical neural network model for visually guided navigation that has been embodied on a physical robot exploring a real-world environment. The model includes a rate based motion energy model for area V1, and a spiking neural network model for cortical area MT. The model generates a cortical representation of optic flow, determines the position of objects based on motion discontinuities, and combines these signals with the representation of a goal location to produce motor commands that successfully steer the robot around obstacles toward the goal. The model produces robot trajectories that closely match human behavioral data. This study demonstrates how neural signals in a model of cortical area MT might provide sufficient motion information to steer a physical robot on human-like paths around obstacles in a real-world environment, and exemplifies the importance of embodiment, as behavior is deeply coupled not only with the underlying model of brain function, but also with the anatomical constraints of the physical body it controls.","tags":[],"title":"A GPU-accelerated cortical neural network model for visually guided robot navigation","type":"publication"},{"authors":["**M Beyeler**","KD Carlson","T-S Chou","N Dutt","JL Krichmar"],"categories":null,"content":"","date":1436684400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541791001,"objectID":"3143c574fcf8286ea71faab0619bab12","permalink":"https://mbeyeler.github.io/publication/carlsim3/","publishdate":"2015-07-12T00:00:00-07:00","relpermalink":"/publication/carlsim3/","section":"publication","summary":"Spiking neural network (SNN) models describe key aspects of neural function in a computationally efficient manner and have been used to construct large-scale brain models. Large-scale SNNs are challenging to implement, as they demand high-bandwidth communication, a large amount of memory, and are computationally intensive. Additionally, tuning parameters of these models becomes more difficult and time-consuming with the addition of biologically accurate descriptions. To meet these challenges, we have developed CARLsim 3, a user-friendly, GPU-accelerated SNN library written in C/C++ that is capable of simulating biologically detailed neural models. The present release of CARLsim provides a number of improvements over our prior SNN library to allow the user to easily analyze simulation data, explore synaptic plasticity rules, and automate parameter tuning. In the present paper, we provide examples and performance benchmarks highlighting the library's features.","tags":[],"title":"CARLsim 3: A user-friendly and highly optimized library for the creation of neurobiologically detailed spiking neural networks","type":"publication"},{"authors":["**M Beyeler**","F Mirus","A Verl"],"categories":null,"content":"","date":1401606000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541791001,"objectID":"9ab9ec5b516f2cb565169a7d068092b6","permalink":"https://mbeyeler.github.io/publication/vision-road-lane-detection/","publishdate":"2014-06-01T00:00:00-07:00","relpermalink":"/publication/vision-road-lane-detection/","section":"publication","summary":"Road and lane detection play an important role in autonomous driving and commercial driver-assistance systems. Vision-based road detection is an essential step towards autonomous driving, yet a challenging task due to illumination and complexity of the visual scenery. Urban scenes may present additional challenges such as intersections, multi-lane scenarios, or clutter due to heavy traffic. This paper presents an integrative approach to ego-lane detection that aims to be as simple as possible to enable real-time computation while being able to adapt to a variety of urban and rural traffic scenarios. The approach at hand combines and extends a road segmentation method in an illumination-invariant color image, lane markings detection using a ridge operator, and road geometry estimation using RANdom SAmple Consensus (RANSAC). Employing the segmented road region as a prior for lane markings extraction significantly improves the execution time and success rate of the RANSAC algorithm, and makes the detection of weakly pronounced ridge structures computationally tractable, thus enabling ego-lane detection even in the absence of lane markings. Segmentation performance is shown to increase when moving from a color-based to a histogram correlation-based model. The power and robustness of this algorithm has been demonstrated in a car simulation system as well as in the challenging KITTI data base of real-world urban traffic scenarios.","tags":[],"title":"Vision-based robust road lane detection in urban environments","type":"publication"},{"authors":["**M Beyeler**","M Richert","ND Dutt","JL Krichmar"],"categories":null,"content":"","date":1391587200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541791001,"objectID":"83c673e82013204e9ba089eb76f6492e","permalink":"https://mbeyeler.github.io/publication/snn-pattern-motion/","publishdate":"2014-02-05T00:00:00-08:00","relpermalink":"/publication/snn-pattern-motion/","section":"publication","summary":"Simulating large-scale models of biological motion perception is challenging, due to the required memory to store the network structure and the computational power needed to quickly solve the neuronal dynamics. A low-cost yet high-performance approach to simulating large-scale neural network models in real-time is to leverage the parallel processing capability of graphics processing units (GPUs). Based on this approach, we present a two-stage model of visual area MT that we believe to be the first large-scale spiking network to demonstrate pattern direction selectivity. In this model, component-direction-selective (CDS) cells in MT linearly combine inputs from V1 cells that have spatiotemporal receptive fields according to the motion energy model of Simoncelli and Heeger. Pattern-direction-selective (PDS) cells in MT are constructed by pooling over MT CDS cells with a wide range of preferred directions. Responses of our model neurons are comparable to electrophysiological results for grating and plaid stimuli as well as speed tuning. The behavioral response of the network in a motion discrimination task is in agreement with psychophysical data. Moreover, our implementation outperforms a previous implementation of the motion energy model by orders of magnitude in terms of computational speed and memory usage. The full network, which comprises 153,216 neurons and approximately 40 million synapses, processes 20 frames per second of a 40 × 40 input video in real-time using a single off-the-shelf GPU. To promote the use of this algorithm among neuroscientists and computer vision researchers, the source code for the simulator, the network, and analysis scripts are publicly available.","tags":[],"title":"Efficient spiking neural network model of pattern motion selectivity in visual cortex","type":"publication"},{"authors":["KD Carlson","**M Beyeler**","N Dutt","JL Krichmar"],"categories":null,"content":"","date":1390204800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541791001,"objectID":"5f90afbf7be9a975cfb19dc21e3bf9cd","permalink":"https://mbeyeler.github.io/publication/gpgpu-accelerated-simulation-parameter-tuning/","publishdate":"2014-01-20T00:00:00-08:00","relpermalink":"/publication/gpgpu-accelerated-simulation-parameter-tuning/","section":"publication","summary":"Neuromorphic engineering takes inspiration from biology to design brain-like systems that are extremely low-power, fault-tolerant, and capable of adaptation to complex environments. The design of these artificial nervous systems involves both the development of neuromorphic hardware devices and the development neuromorphic simulation tools. In this paper, we describe a simulation environment that can be used to design, construct, and run spiking neural networks (SNNs) quickly and efficiently using graphics processing units (GPUs). We then explain how the design of the simulation environment utilizes the parallel processing power of GPUs to simulate large-scale SNNs and describe recent modeling experiments performed using the simulator. Finally, we present an automated parameter tuning framework that utilizes the simulation environment and evolutionary algorithms to tune SNNs. We believe the simulation environment and associated parameter tuning framework presented here can accelerate the development of neuromorphic software and hardware applications by making the design, construction, and tuning of SNNs an easier task.","tags":[],"title":"GPGPU accelerated simulation and parameter tuning for neuromorphic applications","type":"publication"},{"authors":["**M Beyeler**","ND Dutt","JL Krichmar"],"categories":null,"content":"","date":1385884800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541791001,"objectID":"ebbd6461b617fc842472c17c35414d5b","permalink":"https://mbeyeler.github.io/publication/categorization-mnist-stdp/","publishdate":"2013-12-01T00:00:00-08:00","relpermalink":"/publication/categorization-mnist-stdp/","section":"publication","summary":"Understanding how the human brain is able to efficiently perceive and understand a visual scene is still a field of ongoing research. Although many studies have focused on the design and optimization of neural networks to solve visual recognition tasks, most of them either lack neurobiologically plausible learning rules or decision-making processes. Here we present a large-scale model of a hierarchical spiking neural network (SNN) that integrates a low-level memory encoding mechanism with a higher-level decision process to perform a visual classification task in real-time. The model consists of Izhikevich neurons and conductance-based synapses for realistic approximation of neuronal dynamics, a spike-timing-dependent plasticity (STDP) synaptic learning rule with additional synaptic dynamics for memory encoding, and an accumulator model for memory retrieval and categorization. The full network, which comprised 71,026 neurons and approximately 133 million synapses, ran in real-time on a single off-the-shelf graphics processing unit (GPU). The network was constructed on a publicly available SNN simulator that supports general-purpose neuromorphic computer chips. The network achieved 92% correct classifications on MNIST in 100 rounds of random sub-sampling, which is comparable to other SNN approaches and provides a conservative and reliable performance metric. Additionally, the model correctly predicted reaction times from psychophysical experiments. Because of the scalability of the approach and its neurobiological fidelity, the current model can be extended to an efficient neuromorphic implementation that supports more generalized object recognition and decision-making architectures found in the brain.","tags":[],"title":"Categorization and decision-making in a neurobiologically plausible spiking network using a STDP-like learning rule","type":"publication"},{"authors":["**M Beyeler**","F Stefanini","H Proske","G Galizia","E Chicca"],"categories":null,"content":"","date":1288594800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541791001,"objectID":"38f403af7b1100d24f586ef798519e24","permalink":"https://mbeyeler.github.io/publication/exploring-olfactory-networks/","publishdate":"2010-11-01T00:00:00-07:00","relpermalink":"/publication/exploring-olfactory-networks/","section":"publication","summary":"Olfactory stimuli are represented in a high-dimensional space by neural networks of the olfactory system. A great deal of research in olfaction has focused on this representation within the first processing stage, the olfactory bulb (vertebrates) or antennal lobe (insects) glomeruli. In particular the mapping of chemical stimuli onto olfactory glomeruli and the relation of this mapping to perceptual qualities have been investigated. While a number of studies have illustrated the importance of inhibitory networks within the olfactory bulb or the antennal lobe for the shaping and processing of olfactory information, it is not clear how exactly these inhibitory networks are organized to provide filtering and contrast enhancement capabilities. In this work the aim is to study the topology of the proposed networks by using software simulations and hardware implementation. While we can study the dependence of the activity on each parameter of the theoretical models with the simulations, it is important to understand whether the models can be used in robotic applications for real-time odor recognition. We present the results of a linear simulation, a spiking simulation with I\u0026F neurons and a real-time hardware emulation using neuromorphic VLSI chips. We used an input data set of neurophysiological recordings from olfactory receptive neurons of insects, especially Drosophila.","tags":[],"title":"Exploring olfactory sensory networks: Simulations and hardware emulation","type":"publication"}]